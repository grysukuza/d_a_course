{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Shallow Sommelier (Classification, KNN, SVM, Logistic Regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#classification` `#k-nearest-neighbors` `#support-vector-machines` `#logistic-regression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives:\n",
    ">\n",
    "> - Explore classification models\n",
    "> - Use SciKit-Learn's models to perform classification:\n",
    ">   - K-Nearest Neighbors\n",
    ">   - Support Vector Machines\n",
    ">   - Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deep Atlas Exercise Set Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Ensure you are using the coursework Pipenv environment and kernel ([instructions](../SETUP.md))\n",
    "- [ ] Apply the standard Deep Atlas environment setup process by running this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join('..', 'includes'))\n",
    "\n",
    "import deep_atlas\n",
    "from deep_atlas import FILL_THIS_IN\n",
    "deep_atlas.initialize_environment()\n",
    "if deep_atlas.environment == 'COLAB':\n",
    "    %pip install -q python-dotenv==1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš¦ Checkpoint: Start\n",
    "\n",
    "- [ ] Run this cell to record your start time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_atlas.log_start_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many ML applications involve assigning predefined labels (classes) to input data by recognizing patterns. Models are trained on labeled data to classify unseen data.\n",
    "\n",
    "Common applications include:\n",
    "\n",
    "- Spam detection\n",
    "- Sentiment analysis\n",
    "- Image recognition\n",
    "- Fraud detection\n",
    "- Medical diagnosis\n",
    "\n",
    "Large datasets may require deep learning, but this walkthrough covers shallow learning techniques:\n",
    "\n",
    "- **K-Nearest Neighbors (KNN)**: Classifies instances based on the classes of their k-nearest neighbors.\n",
    "- **Support Vector Machines (SVM)**: Finds a plane that maximally separates classes in the feature space.\n",
    "- **Logistic Regression**: Models the probability of class membership using a logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Goal:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Develop a model that can classify wine varieties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deep_atlas.environment == 'VIRTUAL':\n",
    "    !pipenv install ipykernel==6.28.0\n",
    "    !pipenv install scikit-learn==1.4.1.post1 pandas==2.2.1 matplotlib==3.8.3\n",
    "if deep_atlas.environment == 'COLAB':\n",
    "    %pip install scikit-learn==1.4.1.post1 pandas==2.2.1 matplotlib==3.8.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "import random\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Creating training/testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Inspecting data\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Suppress scientific notation in printed output\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we can load data using SciKit-Learn's built-in \"load_wine\" function, one of a [few easily loaded practice datasets](https://scikit-learn.org/stable/datasets.html#datasets).\n",
    "\n",
    "- [ ] Get the features (X) and the corresponding classes (y) from the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset\n",
    "X, y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the dataset contains the following features: alcohol content, malic acid, ash, alcalinity of ash, magnesium, total phenols, flavanoids, nonflavanoid phenols, proanthocyanins, color intensity, hue, dilution amount, proline\n",
    "\n",
    "The dataset has some feature engineering applied to it already: all the values are numeric and there are no missing values to be interpolated.\n",
    "\n",
    "- [ ] Print 5 random samples from the training dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random set of 5 items from the dataset:\")\n",
    "random_indices = random.sample(range(len(X)), 5)\n",
    "for i in random_indices:\n",
    "    print(f\"Label: {y[i]}\")\n",
    "    print(f\"Features: {X[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a few samples helps understand the types of features, but does not give us an intuitive view of the data.\n",
    "\n",
    "Instead, lets try applying Principle Component Analysis to find the 2 features which create most separation in the data. We can then plot the points along those axes, coloring each point by its label.\n",
    "\n",
    "- [ ] Perform PCA and plot the points:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the classes after PCA\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"Wine Dataset - Classes after PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in setting up the data for training is creating a training and testing split:\n",
    "\n",
    "- [ ] Set aside 20% of the data for testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=FILL_THIS_IN, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Solution:</summary>\n",
    "\n",
    "```py\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SciKit-Learn Classification Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major benefit of using the established SciKit-Learn library is that all of its model implementations expose the same APIs (with methods like `fit` for training and `predict` for inference).\n",
    "\n",
    "This allows us to write a single training function to fit a few different models.\n",
    "\n",
    "- [ ] In the function definition below:\n",
    "  - Update the function signature to accept the name and classifier instance as arguments.\n",
    "  - Update the print statement to print the name of the model being explored.\n",
    "  - Call the classifier's `fit` method after the start time has been recorded.\n",
    "    - Make sure to pass in the training features (`X_train`) and labels (`y_train`).\n",
    "  - Call the classifier's `predict` method and set its output to `y_pred`.\n",
    "  - Save each model instance, accuracy and training time to the `results` dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the trained model, accuracy, and training time\n",
    "results = {}\n",
    "\n",
    "\n",
    "def train_and_evaluate(FILL_THIS_IN):\n",
    "    print(f\"Training {FILL_THIS_IN}...\")\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    start_time = time.time()\n",
    "\n",
    "    FILL_THIS_IN\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")\n",
    "\n",
    "    # Predict using the testing data\n",
    "    y_pred = FILL_THIS_IN\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Store the results\n",
    "    results[FILL_THIS_IN] = {\n",
    "        \"model\": FILL_THIS_IN,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Training Time\": training_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of classification models is typically done using metrics such as accuracy, precision, recall, and F1 score. These metrics provide insights into the performance of the model in terms of correctly classifying instances from different classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Solution:</summary>\n",
    "\n",
    "```py\n",
    "def train_and_evaluate(name, classifier):\n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    start_time = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")\n",
    "\n",
    "    # Predict using the testing data\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Store the results\n",
    "    results[name] = {\n",
    "        \"model\": classifier,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Training Time\": training_time,\n",
    "    }\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbors (KNN) algorithm is a type of instance-based learning algorithm.\n",
    "\n",
    "Here's how it works for classification:\n",
    "\n",
    "1. During the training phase, the KNN algorithm simply stores all the training data.\n",
    "\n",
    "2. When you want to classify a new, unseen instance, the KNN algorithm finds the `k` training instances that are closest to the new instance. \"Closeness\" is typically measured using a distance metric, such as Euclidean distance.\n",
    "\n",
    "3. **Majority Voting**: The algorithm then assigns the class label of the new instance based on the majority class label of these `k` nearest neighbors. In other words, the new instance is assigned to the class that most of its `k` nearest neighbors belong to.\n",
    "\n",
    "The number `k` is a hyperparameter that you choose. A small `k` (like 1 or 2) will make the classifier more sensitive to noise in the data, while a large `k` will make the classifier more resistant to noise, but also more likely to misclassify instances because it considers more distant instances in the voting process.\n",
    "\n",
    "The KNN algorithm is simple and can be very effective, but it can also be slow for large datasets because it needs to compute the distance between the new instance and every instance in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(\n",
    "    \"K-Nearest Neighbors\",\n",
    "    KNeighborsClassifier(n_neighbors=5, weights=\"uniform\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Note the training times and accuracy of this model and the subsequent ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are a class of supervised algorithms for both classification and regression. In Scikit-Learn, it's implemented in the `SVC` (Support Vector Classification) and `SVR` (Support Vector Regression) classes.\n",
    "\n",
    "Here's how it works for classification:\n",
    "\n",
    "1. **Training**: During the training phase, the SVM algorithm tries to find a hyperplane\\* that separates the classes in the feature space. If the data is not linearly separable, it uses a technique called the kernel trick\\*\\* to project the data into a higher-dimensional space where a hyperplane can be found. The chosen hyperplane is the one that maximizes the margin between the classes, which is defined as the distance between the hyperplane and the closest data points from each class (these points are called support vectors).\n",
    "\n",
    "   - \\*_Hyperplane_ refers to a plane in a high-dimensional space that is one dimensional lower than the space that it's in. For example, in a 3D space, a hyperplane would look like a suspended 2D plane.\n",
    "   - \\*\\*_Kernel trick_ is a function used to compute the dot-product of two vectors in a higher-dimensional space.\n",
    "\n",
    "2. **Prediction**: When you want to classify a new, unseen instance, the SVM algorithm applies the same transformation to the new instance as it did to the training data (if a kernel was used), and then determines which side of the hyperplane the new instance falls on. The class of the new instance is then determined based on which side of the hyperplane it falls on.\n",
    "\n",
    "The SVM algorithm is effective in high-dimensional spaces and best suited for problems where the number of dimensions is greater than the number of samples. It's also versatile as different Kernel functions can be specified for the decision function. However, it does not directly provide probability estimates.\n",
    "\n",
    "See the resources section below for a visual explanation of SVMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(\"Support Vector Machine\", SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its basic form, Logistic Regression trains a function to model classify an input into two classes (binary classification).\n",
    "\n",
    "Here's how it works for classification:\n",
    "\n",
    "1. **Training**: During the training phase, the Logistic Regression algorithm tries to find the best parameters (weights and bias) for the logistic function that separates the classes in the feature space. This is done by minimizing a cost function (like the log loss) using an optimization algorithm (like Gradient Descent). The logistic function is an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits. This output can be interpreted as the probability of the instance belonging to the positive class.\n",
    "\n",
    "2. **Prediction**: When you want to classify a new, unseen instance, the Logistic Regression algorithm applies the logistic function to the dot product of the instance features and the learned weights, plus the bias term. If the output is greater than 0.5, the instance is classified as the positive class. Otherwise, it's classified as the negative class.\n",
    "\n",
    "Thus, it has the advantage of providing probabilities for the predictions, which can be useful in many applications.\n",
    "\n",
    "#### What about _multi-class_ problems?\n",
    "\n",
    "Logistic Regression can be extended to handle multi-class classification problems:\n",
    "\n",
    "1. **One-vs-Rest (OvR)**: In this strategy, a separate model is trained for each class predicted against all other classes. For example, if there are three classes A, B, and C, three models would be trained: A vs. B and C, B vs. A and C, and C vs. A and B. To make a prediction, all models are run on the input and the model with the highest confidence in its prediction is chosen.\n",
    "2. **Softmax/Multinomial Logistic Regression**: This is a generalization of Logistic Regression to the multi-class case. The model computes a score for each class, then applies the softmax function to these scores to obtain the probability of each class. The class with the highest probability is chosen as the prediction. The model is trained by minimizing the cross-entropy loss, which penalizes the model if it estimates a low probability for the target class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(\n",
    "    \"Logistic Regression\",\n",
    "    LogisticRegression(\n",
    "        max_iter=200, solver=\"lbfgs\", multi_class=\"multinomial\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Note the convergence warning. The SciKit-Learn LogisticRegression class does not use Gradient Descent as its solver by default; in this case, the solver was not able to get to a minimum loss because the training loop ran out of iterations.\n",
    "- [ ] (Optional) Try fixing the problem following the recommendations in the error message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš¦ Checkpoint: Stop\n",
    "\n",
    "- [ ] Uncomment this code\n",
    "- [ ] Complete the feedback form\n",
    "- [ ] Run the cell to log your responses and record your stop time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_atlas.log_feedback(\n",
    "#     {\n",
    "#         # How long were you actively focused on this section? (HH:MM)\n",
    "#         \"active_time\": FILL_THIS_IN,\n",
    "#         # Did you feel finished with this section (Yes/No):\n",
    "#         \"finished\": FILL_THIS_IN,\n",
    "#         # How much did you enjoy this section? (1â€“5)\n",
    "#         \"enjoyment\": FILL_THIS_IN,\n",
    "#         # How useful was this section? (1â€“5)\n",
    "#         \"usefulness\": FILL_THIS_IN,\n",
    "#         # Did you skip any steps?\n",
    "#         \"skipped_steps\": [FILL_THIS_IN],\n",
    "#         # Any obvious opportunities for improvement?\n",
    "#         \"suggestions\": [FILL_THIS_IN],\n",
    "#     }\n",
    "# )\n",
    "# deep_atlas.log_stop_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You did it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "- [Classifier comparison â€” SciKit-Learn docs](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "- Video: [Support Vector Machine (SVM) in 2 minutes](https://www.youtube.com/watch?v=_YPScrckx28)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesson_04-01_attention-JKPCIsR9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
