{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Appraiser, (XGBoost, Regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#extreme-gradient-boosting` `#decision-trees` `#ensemble-learning` `#regression` `#cross-validation` `#hyperparameter-tuning `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives\n",
    ">\n",
    "> - Implement a new version of the House Appraiser model with support for multiple features using XGBoost.\n",
    "> - Use SciKit-Learn's Pipeline functionality for data preprocessing and model evaluation.\n",
    "> - Perform Cross-Validation, a technique for gauging model generalizability and comparing model.\n",
    "> - Use Hyperparameter Tuning to optimize training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deep Atlas Exercise Set Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Ensure you are using the coursework Pipenv environment and kernel ([instructions](../SETUP.md))\n",
    "- [ ] Apply the standard Deep Atlas environment setup process by running this cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join('..', 'includes'))\n",
    "\n",
    "import deep_atlas\n",
    "from deep_atlas import FILL_THIS_IN\n",
    "deep_atlas.initialize_environment()\n",
    "if deep_atlas.environment == 'COLAB':\n",
    "    %pip install -q python-dotenv==1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš¦ Checkpoint: Start\n",
    "\n",
    "- [ ] Run this cell to record your start time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_atlas.log_start_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (XGB) refers to the technique \"eXtreme Gradient Boosting\".\n",
    "\n",
    "It is a library that builds on SciKit-Learn's shallow-modeling capabilities. We will still need to use SciKit-Learn's APIs _alongside_ XGB for tasks outside of model training itself (data processing etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does XGBoost do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (XGB) implements decision trees, a supervised learning model:\n",
    "\n",
    "A decision tree has nodes (features), branches (decisions), and leaves (predictions). Data is recursively split based on a loss function, and predictions are made by traversing the tree.\n",
    "\n",
    "> Contrast with _random forests_: ensembles of decision trees that run in parallel on different subsets of data and features. They aggregate votes from individual trees to reduce overfitting and underfitting. Gradient boosting is distinct _ensemble_ strategy.\n",
    "\n",
    "Gradient Boosting:\n",
    "\n",
    "- Predicts by summing outputs of many models, added sequentially to correct errors.\n",
    "- Models are added until no further improvements can be made.\n",
    "\n",
    "Pros of XGBoost:\n",
    "\n",
    "- Efficient API for gradient boosting.\n",
    "- Supports regression, classification, ranking.\n",
    "- Handles missing values, categorical data, and regularization.\n",
    "- Includes cross-validation, feature importance, and can integrate with scikit-learn utilities like GridSearchCV.\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Not suited for deep learning tasks (e.g., transformers, GANs, reinforcement learning). Use PyTorch for these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Goals:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this exercise we will be using XGBoost's Regression model to predict a house's price, given other features about the house.\n",
    "  - This model will be able to perform multiple regression â€” using multiple features to perform prediction â€”Â like you did with SciKit-Learn in previous exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deep_atlas.environment == 'VIRTUAL': \n",
    "    !pipenv install xgboost==2.0.3 ipykernel==6.28.0 pandas==2.1.4 scikit-learn==1.3.2 \n",
    "if deep_atlas.environment == 'COLAB':\n",
    "    %pip install xgboost==2.0.3 ipykernel==6.28.0 pandas==2.1.4 scikit-learn==1.3.2 matplotlib==3.8.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # interface for data loading\n",
    "import matplotlib.pyplot as plt  # visualization\n",
    "from sklearn.model_selection import train_test_split  # splitting data\n",
    "from sklearn.metrics import mean_absolute_error  # evaluation metric\n",
    "from sklearn.compose import ColumnTransformer  # processing columns\n",
    "from sklearn.preprocessing import OneHotEncoder  # processing string data\n",
    "from sklearn.impute import SimpleImputer  # processing missing data\n",
    "from sklearn.pipeline import Pipeline  # pipeline constructor\n",
    "from sklearn.model_selection import cross_val_score  # cross-validation\n",
    "from sklearn.model_selection import GridSearchCV  # hyperparameter tuning\n",
    "from sklearn.ensemble import RandomForestRegressor  # random forest\n",
    "from xgboost import XGBRegressor  # gradient boosted trees\n",
    "import xgboost as xgb  # model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the data\n",
    "\n",
    "- [ ] Explore the data in `housing.csv` and note the available features.\n",
    "- [ ] Specify the columns to use while training\n",
    "  - [ ] We will not use `ocean_proximity` yet; some preprocessing is required before XGBoost can consume the values in that field without breaking.\n",
    "- [ ] Split the dataset into subsets using SciKit-Learn's `train_test_split`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./assets/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "]  # without the feature \"ocean_proximity\"\n",
    "\n",
    "X = data[cols_to_use]\n",
    "\n",
    "y = data[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the XGBRegressor model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The building block of the XGBRegressor model is a decision tree â€”Â a function that produces a probable output based on a particular feature.\n",
    "\n",
    "XGBRegressor is a class provided by XGBoost that mimics the interface of SciKit-Learns RandomForestRegressor but uses gradient boosting instead (more models are added in sequence until no improvements can be made).\n",
    "\n",
    "- [ ] Train the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(\n",
    "    n_estimators=500,  # Rough number of trees to use.\n",
    "    early_stopping_rounds=5,  # Rounds of no improvement before stopping.\n",
    "    learning_rate=0.01,  # How much each tree should adjust the answer.\n",
    "    n_jobs=4,  # Parallel processing if available to the computer.\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Use the model to predict values in the test data and note the mean error in terms of dollars-away-from-actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\n",
    "    \"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_test))\n",
    ")\n",
    "plt.scatter(predictions, y_test, alpha=0.1)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Actual Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: A great feature of XGBoost's model is interpretability: its ability to report the relative importance of each feature (the number of times a feature was used to split the data in the branching decision tree). This is referred to as the F-score of a feature.\n",
    "\n",
    "- [ ] Plot the importance of the features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš¦ Checkpoint: Stop\n",
    "\n",
    "- [ ] Uncomment this code\n",
    "- [ ] Complete the feedback form\n",
    "- [ ] Run the cell to log your responses and record your stop time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_atlas.log_feedback(\n",
    "#     {\n",
    "#         # How long were you actively focused on this section? (HH:MM)\n",
    "#         \"active_time\": FILL_THIS_IN,\n",
    "#         # Did you feel finished with this section (Yes/No):\n",
    "#         \"finished\": FILL_THIS_IN,\n",
    "#         # How much did you enjoy this section? (1â€“5)\n",
    "#         \"enjoyment\": FILL_THIS_IN,\n",
    "#         # How useful was this section? (1â€“5)\n",
    "#         \"usefulness\": FILL_THIS_IN,\n",
    "#         # Did you skip any steps?\n",
    "#         \"skipped_steps\": [FILL_THIS_IN],\n",
    "#         # Any obvious opportunities for improvement?\n",
    "#         \"suggestions\": [FILL_THIS_IN],\n",
    "#     }\n",
    "# )\n",
    "# deep_atlas.log_stop_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You did it!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesson_02_decision_trees-IDKbApM_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
