{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: MNIST Visualizer (Dimensionality Reduction, PCA, t-SNE, UMAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#dimensionality-reduction` `#pca` `#t-sne` `#umap` `#matplotlib` `#visualization`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives:\n",
    ">\n",
    "> - Explore how data embeddings in a high-dimensional space can be visualized in 2-dimensional mappings using the following methods:\n",
    ">   - PCA (Principal Component Analysis)\n",
    ">   - TSNE (t-Distributed Stochastic Neighbor Embedding)\n",
    ">   - UMAP (Uniform Manifold Approximation and Projection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deep Atlas Exercise Set Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Ensure you are using the coursework Pipenv environment and kernel ([instructions](../SETUP.md))\n",
    "- [ ] Apply the standard Deep Atlas environment setup process by running this cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join('..', 'includes'))\n",
    "\n",
    "import deep_atlas\n",
    "from deep_atlas import FILL_THIS_IN\n",
    "deep_atlas.initialize_environment()\n",
    "if deep_atlas.environment == 'COLAB':\n",
    "    %pip install -q python-dotenv==1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš¦ Checkpoint: Start\n",
    "\n",
    "- [ ] Run this cell to record your start time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_atlas.log_start_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous exercises, we converted features into high-dimensional embeddings, useful for techniques like vector search and generative AI, but difficult to visualize.\n",
    "\n",
    "As an ML engineer, you'll need to reduce high-dimensional data to 2D to inspect clusters and patterns. This exercise explores dimensionality reduction techniques using SciKit-Learn, without delving into the underlying math.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deep_atlas.environment == 'VIRTUAL':\n",
    "    !pipenv install matplotlib==3.8.2 scikit-learn umap-learn\n",
    "if deep_atlas.environment == 'COLAB':\n",
    "    %pip install matplotlib==3.8.2 scikit-learn umap-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and process the MNIST dataset, which contains 28Ã—28px black-and-white scans of handwritten digits (0â€“9), each represented by 784 values from 0 (black) to 255 (white).\n",
    "\n",
    "SciKit-Learn, like TensorFlow, PyTorch, and fast.ai, provides an API to download popular datasets. Keep in mind that for real-world applications, you'll need to source, clean, and engineer your own datasets.\n",
    "\n",
    "For this exercise, we can download the dataset using the the `fetch_openml` function (imported above):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "\n",
    "# Convert each pixel value to a float between 0 and 1\n",
    "X, y = mnist.data / 255.0, mnist.target\n",
    "\n",
    "# MNIST is a large dataset, containing 70,000 images.\n",
    "# Reduce the dataset size for quicker execution:\n",
    "X, y = X[:10000], y[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting values\n",
    "\n",
    "In order to see the output of the dimensionality reduction, define a function which will plot data in two dimensions:\n",
    "\n",
    "Tip: Note how the `plot_embeddings` function is being invoked below and return to this cell to understand the plots being rendered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding(data, y, title):\n",
    "    # `data` is a 2D array of shape (n_samples, 2)\n",
    "    # `y` is a 1D array of shape (n_samples,), representing the labels\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(\n",
    "        # Value of data in the first dimension\n",
    "        data[:, 0],\n",
    "        # Value of data in the second dimension\n",
    "        data[:, 1],\n",
    "        # Color of each point, representing the label\n",
    "        c=y.astype(int),\n",
    "        # Use a categorical color map with 10 distinct colors\n",
    "        cmap=\"tab10\",\n",
    "        # size of each point\n",
    "        s=1,\n",
    "    )\n",
    "    # Add a color bar to the right of the plot\n",
    "    plt.colorbar(scatter)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA:Â Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA's transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is linearly uncorrelated (i.e. orthogonal) to the preceding components.\n",
    "\n",
    "PCA is particularly effective in scenarios where:\n",
    "\n",
    "1. There are linear correlations between variables in your data.\n",
    "2. The dataset has high dimensionality but you suspect that many of the features are redundant or irrelevant.\n",
    "3. You want to perform data compression while maintaining the structure and complexity of the data.\n",
    "\n",
    "Limitations\n",
    "\n",
    "1. **Larger variance != more interesting**: PCA assumes that the component (or direction in feature space) with the largest variance is the most \"interesting\". This may not always be the case, and sometimes the components with smaller variance may also contain important information.\n",
    "1. **Orthogonality**: The decision to make principal components orthogonal may not always make sense. This could be a limitation if the components are not really orthogonal in your data.\n",
    "1. **Scaling**: PCA is sensitive to the scaling of your variables. If you have variables with large values, they may end up dominating the first principal component when they should not. It's often a good idea to normalize your data before applying PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and project principal components\n",
    "X_pca = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "# Plot the PCA projection\n",
    "plot_embedding(X_pca, y, title=\"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE:Â t-Distributed Stochastic Neighbor Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE models each high-dimensional input vector by a two- or three-dimensional point in such a way that similar instances land close by each other and dissimilar instances are moved apart.\n",
    "\n",
    "It is particularly effective in scenarios where:\n",
    "\n",
    "1. You want to visualize clusters in your data, as t-SNE can separate clusters quite well in the low-dimensional space.\n",
    "1. The structure of the data at various scales is of interest, as t-SNE can capture structure at different scales.\n",
    "\n",
    "Limitations\n",
    "\n",
    "1. **Hyperparameters**: t-SNE has a few hyperparameters (like perplexity and learning rate) that can significantly affect the resulting visualization. It might require some trial and error to find the best settings.\n",
    "1. **Global vs. Local Structure**: t-SNE is particularly good at preserving local structure in the data (meaning it keeps similar instances close together), but it doesn't preserve the global structure as well. This means that the distance between widely separated clusters in the t-SNE plot may not mean anything.\n",
    "1. **Randomness**: t-SNE uses a random initialization as part of its algorithm, which means that you can get different results every time you run it. This can make it hard to interpret the results. In the code below, this is countered by setting a seed (`random_state`).\n",
    "1. **No Inverse Mapping**: Unlike PCA, t-SNE does not provide an explicit function to map new, unseen data into the same space.\n",
    "1. **Computational Complexity**: t-SNE has a high computational complexity, making it less suitable for very large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 784-dimensional data as 2-dimensional data,\n",
    "# placing similar points close together\n",
    "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)\n",
    "\n",
    "# Plot the t-SNE projection\n",
    "plot_embedding(X_tsne, y, title=\"t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP: Uniform Manifold Approximation Projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP is based on manifold learning techniques. It's primary advantage over t-SNE is that it preserves more of the global structure.\n",
    "\n",
    "> In the context of UMAP and other dimensionality reduction techniques, a manifold refers to a shape or structure in high-dimensional space that can be approximated as a lower-dimensional space locally.\n",
    ">\n",
    "> For example, consider a piece of paper: it's a 2D object living in our 3D world. If you crumple that piece of paper into a ball, it's still a 2D surface, but now it's embedded in 3D space in a complex way. That crumpled piece of paper is an example of a 2D manifold in 3D space.\n",
    ">\n",
    "> In the case of UMAP, it assumes that the high-dimensional data lies on a manifold, and it tries to learn the structure of this manifold. It then uses this learned structure to project the data into a lower-dimensional space in a way that preserves as much of the original data structure as possible. This is why UMAP is particularly good at preserving both local and global structures in the data.\n",
    "\n",
    "UMAP is particularly effective in scenarios where:\n",
    "\n",
    "1. You want to preserve the global structure of the data while reducing dimensions.\n",
    "2. You are dealing with very large datasets. UMAP is faster than t-SNE, making it more suitable for larger datasets.\n",
    "3. You want more consistent results. Unlike t-SNE, which can produce different results with different runs due to its randomness, UMAP tends to produce more consistent results.\n",
    "\n",
    "Limitations\n",
    "\n",
    "1. **Complexity**: UMAP is based on some complex mathematical concepts, which can make it harder to reason about than PCA or t-SNE.\n",
    "2. **Hyperparameters**: Like t-SNE, UMAP also has a few key hyperparameters (like the number of neighbors and the minimum distance) that can significantly affect the resulting visualization. It might require some trial and error to find the best settings.\n",
    "3. **No Inverse Mapping**: Similar to t-SNE, UMAP does not provide an explicit function to map new, unseen data into the same space. However, recent versions of UMAP have added some support for this feature.\n",
    "4. **Assumptions**: UMAP makes some assumptions about the data, such as it being uniformly distributed on a Riemannian manifold. If these assumptions are not met, the results may not be meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 784-dimensional data as 2-dimensional data\n",
    "X_umap = umap.UMAP(random_state=42).fit_transform(X)\n",
    "\n",
    "# Plot the UMAP projection\n",
    "plot_embedding(X_umap, y, title=\"UMAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš¦ Checkpoint: Stop\n",
    "\n",
    "- [ ] Uncomment this code\n",
    "- [ ] Complete the feedback form\n",
    "- [ ] Run the cell to log your responses and record your stop time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_atlas.log_feedback(\n",
    "#     {\n",
    "#         # How long were you actively focused on this section? (HH:MM)\n",
    "#         \"active_time\": FILL_THIS_IN,\n",
    "#         # Did you feel finished with this section (Yes/No):\n",
    "#         \"finished\": FILL_THIS_IN,\n",
    "#         # How much did you enjoy this section? (1â€“5)\n",
    "#         \"enjoyment\": FILL_THIS_IN,\n",
    "#         # How useful was this section? (1â€“5)\n",
    "#         \"usefulness\": FILL_THIS_IN,\n",
    "#         # Did you skip any steps?\n",
    "#         \"skipped_steps\": [FILL_THIS_IN],\n",
    "#         # Any obvious opportunities for improvement?\n",
    "#         \"suggestions\": [FILL_THIS_IN],\n",
    "#     }\n",
    "# )\n",
    "# deep_atlas.log_stop_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You did it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These techniques are essential for data scientists when working with high-dimensional data, as they allow for the visualization and understanding of complex patterns that would be impossible to understand in the high-dimensional space.\n",
    "\n",
    "These techniques will be useful as you explore new data sets for your projects and beyond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further exploration:\n",
    "\n",
    "1. [PCA (Principal Component Analysis)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "2. [t-SNE (t-Distributed Stochastic Neighbor Embedding)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
    "3. [UMAP (Uniform Manifold Approximation and Projection)](https://umap-learn.readthedocs.io/en/latest/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesson_04_deep_learning-be4iLG5n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
